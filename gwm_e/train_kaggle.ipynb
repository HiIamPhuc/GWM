{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a018807c",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0734bf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# These cover all dependencies needed by model.py, dataset.py, and training\n",
    "\n",
    "# Fix protobuf version conflict first\n",
    "!pip uninstall -y protobuf\n",
    "!pip install -q protobuf==3.20.3\n",
    "\n",
    "# Install transformers and dependencies\n",
    "!pip install -q transformers>=4.35.0 accelerate sentencepiece\n",
    "\n",
    "# Install torch-geometric\n",
    "!pip install -q torch-geometric torch-scatter torch-sparse --no-deps\n",
    "!pip install -q huggingface-hub tqdm\n",
    "\n",
    "# Note: torch, numpy, pandas, scikit-learn are pre-installed on Kaggle\n",
    "print(\"âœ“ All dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdb955b",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38342182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import json\n",
    "import gc\n",
    "\n",
    "# Check environment\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "# Check if running on Kaggle\n",
    "IS_KAGGLE = os.path.exists('/kaggle')\n",
    "print(f\"Running on Kaggle: {IS_KAGGLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541a08f2",
   "metadata": {},
   "source": [
    "## 3. Configuration (Kaggle-Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af4a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths - UPDATE THESE if using different dataset name\n",
    "if IS_KAGGLE:\n",
    "    DATA_CONFIG = {\n",
    "        'train_jsonl': '/kaggle/input/cora-gwm-data/cora_train_node_data.jsonl',\n",
    "        'test_jsonl': '/kaggle/input/cora-gwm-data/cora_test_node_data.jsonl',\n",
    "        'embedding_path': '/kaggle/input/cora-gwm-data/multi_hop_graph_embedding.pt',\n",
    "    }\n",
    "else:\n",
    "    DATA_CONFIG = {\n",
    "        'train_jsonl': '../data/cora/cora_train_node_data.jsonl',\n",
    "        'test_jsonl': '../data/cora/cora_test_node_data.jsonl',\n",
    "        'embedding_path': '../data/cora/multi_hop_graph_embedding.pt',\n",
    "    }\n",
    "\n",
    "# Model configuration - OPTION 1: 8B with 8-bit quantization (RECOMMENDED)\n",
    "MODEL_CONFIG = {\n",
    "    'llama_model': 'meta-llama/Meta-Llama-3-8B-Instruct',  # 8B model with quantization\n",
    "    'graph_embedding_dim': 2048,\n",
    "    'projector_hidden_dim': 2048,  # Reduced to save memory\n",
    "    'num_hops': 5,\n",
    "    'use_8bit': True,  # Enable 8-bit quantization\n",
    "}\n",
    "\n",
    "# OPTION 2: Use 3B model without quantization (comment out above, use this)\n",
    "# MODEL_CONFIG = {\n",
    "#     'llama_model': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "#     'graph_embedding_dim': 2048,\n",
    "#     'projector_hidden_dim': 2048,\n",
    "#     'num_hops': 5,\n",
    "#     'use_8bit': False,\n",
    "# }\n",
    "\n",
    "# Training configuration - OPTIMIZED FOR KAGGLE\n",
    "TRAINING_CONFIG = {\n",
    "    'batch_size': 2,  # Small batch for 8B model\n",
    "    'gradient_accumulation_steps': 16,  # Effective batch = 32\n",
    "    'learning_rate': 2e-4,\n",
    "    'num_epochs': 5,  # Reduced for 12-hour limit\n",
    "    'warmup_steps': 50,  # Reduced warmup\n",
    "    'num_workers': 2,  # Reduced workers\n",
    "    'output_dir': '/kaggle/working/checkpoints' if IS_KAGGLE else './checkpoints',\n",
    "    'save_every': 1,\n",
    "    'use_fp16': True,  # Mixed precision training\n",
    "}\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" \"*20 + \"KAGGLE-OPTIMIZED CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nData:\")\n",
    "for k, v in DATA_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"\\nModel (Memory-Optimized):\")\n",
    "for k, v in MODEL_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"\\nTraining (Kaggle-Optimized):\")\n",
    "for k, v in TRAINING_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(f\"\\nDevice: {device}\")\n",
    "print(f\"Effective batch size: {TRAINING_CONFIG['batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902fd814",
   "metadata": {},
   "source": [
    "## 4. Copy Model Files\n",
    "\n",
    "Copy the model architecture files to working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012cea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone GitHub repo and copy model files to working directory\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    print(\"=\"*70)\n",
    "    print(\"Cloning GitHub repository...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Clone your GitHub repo (UPDATE with your repo URL)\n",
    "    GITHUB_REPO = \"https://github.com/HiIamPhuc/GWM.git\"\n",
    "    \n",
    "    # Clone repo\n",
    "    !git clone {GITHUB_REPO} /kaggle/temp/gwm_repo\n",
    "    \n",
    "    # Copy model files from repo to working directory\n",
    "    repo_path = Path(\"/kaggle/temp/gwm_repo/GWM/gwm_e\")\n",
    "    \n",
    "    if repo_path.exists():\n",
    "        print(f\"\\nâœ“ Repository cloned successfully\")\n",
    "        print(f\"Source: {repo_path}\")\n",
    "        \n",
    "        # Copy files\n",
    "        !cp {repo_path}/model.py /kaggle/working/\n",
    "        !cp {repo_path}/dataset.py /kaggle/working/\n",
    "        \n",
    "        print(\"\\nâœ“ Copied model.py\")\n",
    "        print(\"âœ“ Copied dataset.py\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ Repository path not found: {repo_path}\")\n",
    "        print(\"Please check the repository structure\")\n",
    "else:\n",
    "    print(\"Running locally - files should be in current directory\")\n",
    "\n",
    "# Verify files exist\n",
    "required_files = ['model.py', 'dataset.py']\n",
    "missing_files = [f for f in required_files if not os.path.exists(f)]\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\nâŒ Missing files: {missing_files}\")\n",
    "    raise FileNotFoundError(f\"Required files not found: {missing_files}\")\n",
    "else:\n",
    "    print(f\"\\nâœ“ All required files ready:\")\n",
    "    print(f\"  - model.py\")\n",
    "    print(f\"  - dataset.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fca306a",
   "metadata": {},
   "source": [
    "## 5. Load Model with Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63fd678",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hf auth login --token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e3b00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GWM_E\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" \"*20 + \"Loading GWM-E Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize model\n",
    "print(f\"\\nLoading {MODEL_CONFIG['llama_model']}...\")\n",
    "if MODEL_CONFIG.get('use_8bit'):\n",
    "    print(\"Using 8-bit quantization to fit in 16GB GPU...\")\n",
    "print(\"This may take 2-3 minutes...\\n\")\n",
    "\n",
    "model = GWM_E(\n",
    "    llama_model_path=MODEL_CONFIG['llama_model'],\n",
    "    graph_embedding_dim=MODEL_CONFIG['graph_embedding_dim'],\n",
    "    projector_hidden_dim=MODEL_CONFIG['projector_hidden_dim'],\n",
    "    num_hops=MODEL_CONFIG['num_hops'],\n",
    "    freeze_llm=True,\n",
    "    use_8bit=MODEL_CONFIG.get('use_8bit', False),  # Pass 8-bit flag\n",
    ")\n",
    "\n",
    "print(\"âœ“ Model loaded successfully!\")\n",
    "\n",
    "# Calculate parameters\n",
    "llm_params = sum(p.numel() for p in model.llm.parameters()) / 1e9\n",
    "projector_params = sum(p.numel() for p in model.projector.parameters()) / 1e6\n",
    "trainable_params = sum(p.numel() for p in model.projector.parameters() if p.requires_grad) / 1e6\n",
    "\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  LLaMA parameters: {llm_params:.2f}B (frozen)\")\n",
    "print(f\"  Projector parameters: {projector_params:.2f}M (trainable)\")\n",
    "print(f\"  Total trainable: {trainable_params:.2f}M\")\n",
    "\n",
    "# Check GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    print(f\"\\nGPU Memory:\")\n",
    "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"  Reserved: {reserved:.2f} GB\")\n",
    "    print(f\"  Available: {torch.cuda.get_device_properties(0).total_memory / 1e9 - reserved:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037a11c3",
   "metadata": {},
   "source": [
    "## 6. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa4a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import create_dataloaders\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" \"*20 + \"Loading Datasets\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_loader, test_loader = create_dataloaders(\n",
    "    train_jsonl=DATA_CONFIG['train_jsonl'],\n",
    "    test_jsonl=DATA_CONFIG['test_jsonl'],\n",
    "    embedding_path=DATA_CONFIG['embedding_path'],\n",
    "    tokenizer=model.tokenizer,\n",
    "    batch_size=TRAINING_CONFIG['batch_size'],\n",
    "    num_workers=TRAINING_CONFIG['num_workers'],\n",
    "    num_hops=MODEL_CONFIG['num_hops'],\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Datasets loaded successfully!\")\n",
    "print(f\"  Training samples: {len(train_loader.dataset):,}\")\n",
    "print(f\"  Test samples: {len(test_loader.dataset):,}\")\n",
    "print(f\"  Training batches: {len(train_loader):,}\")\n",
    "print(f\"  Test batches: {len(test_loader):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62f7918",
   "metadata": {},
   "source": [
    "## 7. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a26b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\" \"*20 + \"Setting up Training\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(TRAINING_CONFIG['output_dir'])\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save config\n",
    "config_path = output_dir / \"training_config.json\"\n",
    "all_config = {**DATA_CONFIG, **MODEL_CONFIG, **TRAINING_CONFIG}\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(all_config, f, indent=2)\n",
    "print(f\"âœ“ Saved config to: {config_path}\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.projector.parameters(),\n",
    "    lr=TRAINING_CONFIG['learning_rate'],\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "print(f\"âœ“ Optimizer: AdamW (lr={TRAINING_CONFIG['learning_rate']})\")\n",
    "\n",
    "# Scheduler\n",
    "total_steps = len(train_loader) * TRAINING_CONFIG['num_epochs'] // TRAINING_CONFIG['gradient_accumulation_steps']\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=TRAINING_CONFIG['warmup_steps'],\n",
    "    num_training_steps=total_steps,\n",
    ")\n",
    "print(f\"âœ“ Scheduler: Linear warmup + decay\")\n",
    "print(f\"  Total steps: {total_steps:,}\")\n",
    "print(f\"  Warmup steps: {TRAINING_CONFIG['warmup_steps']:,}\")\n",
    "\n",
    "# Mixed precision (optional)\n",
    "scaler = None\n",
    "if TRAINING_CONFIG.get('use_fp16', False) and torch.cuda.is_available():\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    print(\"âœ“ Mixed precision (FP16) enabled\")\n",
    "\n",
    "print(\"\\nâœ“ Training setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82ea06b",
   "metadata": {},
   "source": [
    "## 8. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888bcc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, scheduler, epoch, gradient_accumulation_steps, device, scaler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        multi_hop_embedding = batch['multi_hop_embedding'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Mixed precision forward pass\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits, loss = model(\n",
    "                    multi_hop_embeddings=multi_hop_embedding,\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                )\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            logits, loss = model(\n",
    "                multi_hop_embeddings=multi_hop_embedding,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "            if scaler is not None:\n",
    "                scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.projector.parameters(), max_norm=1.0)\n",
    "            \n",
    "            if scaler is not None:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            \n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * gradient_accumulation_steps\n",
    "        num_batches += 1\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{loss.item() * gradient_accumulation_steps:.4f}\",\n",
    "            'lr': f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "        })\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            multi_hop_embedding = batch['multi_hop_embedding'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits, loss = model(\n",
    "                multi_hop_embeddings=multi_hop_embedding,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            mask = labels != -100\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            correct += ((predictions == labels) & mask).sum().item()\n",
    "            total += mask.sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "print(\"âœ“ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223ab9a8",
   "metadata": {},
   "source": [
    "## 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec499d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\" \"*20 + \"STARTING TRAINING (KAGGLE-OPTIMIZED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_accuracy = 0\n",
    "training_history = []\n",
    "\n",
    "for epoch in range(1, TRAINING_CONFIG['num_epochs'] + 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Epoch {epoch}/{TRAINING_CONFIG['num_epochs']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        epoch=epoch,\n",
    "        gradient_accumulation_steps=TRAINING_CONFIG['gradient_accumulation_steps'],\n",
    "        device=device,\n",
    "        scaler=scaler,\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_accuracy = evaluate(\n",
    "        model=model,\n",
    "        test_loader=test_loader,\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    # Log results\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Epoch {epoch} Results:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Train Loss:     {train_loss:.4f}\")\n",
    "    print(f\"  Test Loss:      {test_loss:.4f}\")\n",
    "    print(f\"  Test Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "    \n",
    "    # Save history\n",
    "    training_history.append({\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_loss,\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': test_accuracy,\n",
    "    })\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint_path = output_dir / f\"projector_epoch_{epoch}.pt\"\n",
    "    model.save_projector(str(checkpoint_path))\n",
    "    print(f\"  âœ“ Saved: {checkpoint_path.name}\")\n",
    "    \n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        best_path = output_dir / \"projector_best.pt\"\n",
    "        model.save_projector(str(best_path))\n",
    "        print(f\"  â­ New best: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "    \n",
    "    # Memory cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "# Save history\n",
    "history_path = output_dir / \"training_history.json\"\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(training_history, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" \"*20 + \"âœ… TRAINING COMPLETED!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "print(f\"Checkpoints: {output_dir}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c629f84",
   "metadata": {},
   "source": [
    "## 10. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf9bb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract metrics\n",
    "epochs = [h['epoch'] for h in training_history]\n",
    "train_losses = [h['train_loss'] for h in training_history]\n",
    "test_losses = [h['test_loss'] for h in training_history]\n",
    "test_accuracies = [h['test_accuracy'] for h in training_history]\n",
    "\n",
    "# Create plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(epochs, train_losses, 'b-o', label='Train Loss')\n",
    "ax1.plot(epochs, test_losses, 'r-o', label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Test Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(epochs, [acc * 100 for acc in test_accuracies], 'g-o', label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Test Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Saved training curves to: {output_dir / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aaddf7",
   "metadata": {},
   "source": [
    "## 11. Summary and Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83935458",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\" \"*25 + \"TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸ“Š Results:\")\n",
    "print(f\"  Best Test Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "print(f\"  Final Train Loss:   {training_history[-1]['train_loss']:.4f}\")\n",
    "print(f\"  Final Test Loss:    {training_history[-1]['test_loss']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ Saved Files:\")\n",
    "for file in sorted(output_dir.glob(\"*\")):\n",
    "    size = os.path.getsize(file) / (1024**2)\n",
    "    print(f\"  â€¢ {file.name} ({size:.1f} MB)\")\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    print(f\"\\nðŸ“¥ Download from Kaggle:\")\n",
    "    print(f\"  1. Go to 'Output' tab (right sidebar)\")\n",
    "    print(f\"  2. Download 'checkpoints/' folder\")\n",
    "    print(f\"  3. Key file: projector_best.pt (~{os.path.getsize(output_dir / 'projector_best.pt') / (1024**2):.1f} MB)\")\n",
    "\n",
    "print(f\"\\nðŸš€ Next Steps:\")\n",
    "print(f\"  1. Download projector_best.pt\")\n",
    "print(f\"  2. Run inference to evaluate on test set\")\n",
    "print(f\"  3. Use projector with any LLaMA model\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1b9860",
   "metadata": {},
   "source": [
    "## Optional: Test a Single Prediction\n",
    "\n",
    "Quick test to see model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa00a434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample from test set\n",
    "test_sample = test_loader.dataset[0]\n",
    "test_conv = test_loader.dataset.conversations[0]\n",
    "\n",
    "print(\"Testing model on a single example...\\n\")\n",
    "print(\"=\"*70)\n",
    "print(\"Sample Input:\")\n",
    "print(\"=\"*70)\n",
    "for turn in test_conv['conversations']:\n",
    "    role = turn['from']\n",
    "    message = turn['value'][:200] + \"...\" if len(turn['value']) > 200 else turn['value']\n",
    "    print(f\"{role.upper()}: {message}\\n\")\n",
    "\n",
    "# Prepare inputs\n",
    "multi_hop_emb = test_sample['multi_hop_embedding'].unsqueeze(0).to(device)\n",
    "input_ids = test_sample['input_ids'].unsqueeze(0).to(device)\n",
    "attention_mask = test_sample['attention_mask'].unsqueeze(0).to(device)\n",
    "\n",
    "# Generate prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        multi_hop_embeddings=multi_hop_emb,\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "\n",
    "# Decode\n",
    "input_length = input_ids.shape[1]\n",
    "generated = model.tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Model Prediction:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"ASSISTANT: {generated}\\n\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gwm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
